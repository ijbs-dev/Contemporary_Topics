{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"yYDMHCHcdWD9"},"outputs":[],"source":["import os\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torchvision\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader, Subset\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sMC8lkeldWD9"},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SS9pAmV-dWD_"},"outputs":[],"source":["def train_model(model, trainloader, valloader, criterion, optimizer, device, num_epochs=5):\n","    history = {\n","        'train_losses': [],\n","        'val_losses': [],\n","        'train_accuracies': [],\n","        'val_accuracies': []\n","    }\n","\n","    for epoch in range(num_epochs):\n","        # Treinamento\n","        model.train()\n","        running_loss = 0.0\n","        correct = 0\n","        total = 0\n","        for i, data in tqdm(enumerate(trainloader, 0), total=len(trainloader)):\n","            inputs, labels = data\n","            inputs, labels = inputs.to(device), labels.to(device)\n","\n","            optimizer.zero_grad()\n","\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","        train_loss = running_loss / len(trainloader)\n","        train_acc = 100 * correct / total\n","        history['train_losses'].append(train_loss)\n","        history['train_accuracies'].append(train_acc)\n","        print(f'Epoch {epoch+1}, Train Loss: {train_loss:.3f}, Train Accuracy: {train_acc:.2f}%')\n","\n","        # Validação\n","        model.eval()\n","        val_running_loss = 0.0\n","        correct = 0\n","        total = 0\n","        with torch.no_grad():\n","            for data in valloader:\n","                inputs, labels = data\n","                inputs, labels = inputs.to(device), labels.to(device)\n","                outputs = model(inputs)\n","                loss = criterion(outputs, labels)\n","                val_running_loss += loss.item()\n","                _, predicted = torch.max(outputs.data, 1)\n","                total += labels.size(0)\n","                correct += (predicted == labels).sum().item()\n","\n","        val_loss = val_running_loss / len(valloader)\n","        val_acc = 100 * correct / total\n","        history['val_losses'].append(val_loss)\n","        history['val_accuracies'].append(val_acc)\n","        print(f'Epoch {epoch+1}, Val Loss: {val_loss:.3f}, Val Accuracy: {val_acc:.2f}%')\n","\n","    print('Treinamento concluído')\n","    return history\n","\n","\n","def plot_history(history):\n","    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n","\n","    ax1.plot(history['train_losses'], label='Train')\n","    ax1.plot(history['val_losses'], label='Validation')\n","    ax1.set_title('Loss')\n","    ax1.legend()\n","\n","    ax2.plot(history['train_accuracies'], label='Train')\n","    ax2.plot(history['val_accuracies'], label='Validation')\n","    ax2.set_title('Accuracy')\n","    ax2.legend()\n","\n","    plt.show()\n","\n","\n","def test_model(model, testloader):\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for data in testloader:\n","            images, labels = data\n","            outputs = model(images)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    print(f'Acurácia da rede na base de teste: {100 * correct / total:.2f}%')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q1oENbNkdWEA"},"outputs":[],"source":["# Baixa e extrai o dataset\n","!curl -L -o data/animais.zip \"https://drive.google.com/uc?export=download&id=16Lll0Slg1unWxAb26AzZqI9sPdB_fYpV\"\n","!unzip data/animais.zip -d data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4LWmpUMIdWEA"},"outputs":[],"source":["# Carregando os datasets\n","transform_aug = transforms.Compose([\n","    transforms.RandomResizedCrop(224),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","])\n","\n","transform_no_aug = transforms.Compose([\n","    transforms.Resize(230),\n","    transforms.CenterCrop(224),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","])\n","\n","# data_dir = 'data/hymenoptera_data'\n","data_dir = \"data/animais\"\n","\n","train_set = datasets.ImageFolder(os.path.join(data_dir, 'train'), transform=transform_no_aug)\n","val_set = datasets.ImageFolder(os.path.join(data_dir, 'val'), transform=transform_no_aug)\n","\n","train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n","val_loader = DataLoader(val_set, batch_size=32, shuffle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FJjI9MYVdWEB"},"outputs":[],"source":["def imshow(img, title=None):\n","    img = img / 2 + 0.5  # unnormalize\n","    npimg = img.numpy()\n","    plt.imshow(np.transpose(npimg, (1, 2, 0)), interpolation='none')\n","    if title is not None:\n","        plt.title(title)\n","    plt.show()\n","\n","# Função para mostrar exemplos\n","def show_examples(loader, title):\n","    images, labels = next(iter(loader))\n","    images = images[:8]\n","    labels = labels[:8]\n","    imshow(torchvision.utils.make_grid(images, nrow=4), title)\n","\n","# Mostrando exemplos com e sem data augmentation\n","show_examples(train_loader, title=\"Com Augmentation\")\n","show_examples(val_loader, title=\"Sem Augmentation\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OaYvocWddWEC"},"outputs":[],"source":["class CNN(nn.Module):\n","    def __init__(self, num_classes=5):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(3, 8, kernel_size=5, stride=2, padding=0)\n","        self.conv2 = nn.Conv2d(8, 16, kernel_size=5, stride=2, padding=0)\n","        # self.conv3 = nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=0)\n","        self.fc1 = nn.Linear(16 * 13 * 13, 256)\n","        self.fc2 = nn.Linear(256, 64)\n","        self.fc3 = nn.Linear(64, num_classes)\n","\n","    def forward(self, x):\n","        x = F.max_pool2d(F.relu(self.conv1(x)), kernel_size=2, stride=2, padding=0)\n","        x = F.max_pool2d(F.relu(self.conv2(x)), kernel_size=2, stride=2, padding=0)\n","        # x = F.max_pool2d(F.relu(self.conv3(x)), kernel_size=2, stride=2, padding=0)\n","        x = x.view(-1, 16 * 13 * 13)\n","        x = torch.relu(self.fc1(x))\n","        x = torch.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x\n","\n","model = CNN(num_classes=2).to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lSPR_2vAdWEC"},"outputs":[],"source":["criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"olkeJTHBdWED"},"outputs":[],"source":["# Treinando a CNN\n","history = train_model(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=30)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IcgSgyoYdWED"},"outputs":[],"source":["plot_history(history)"]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"colab":{"provenance":[{"file_id":"https://github.com/silvaan/topicos_contemporaneos/blob/main/Part%201%20-%20CNNs/07%20-%20Data%20Augmentation.ipynb","timestamp":1723061337073}]}},"nbformat":4,"nbformat_minor":0}